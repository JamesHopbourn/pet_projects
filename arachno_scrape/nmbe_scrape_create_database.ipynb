{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from lxml.html import fromstring\n",
    "from itertools import cycle\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_simple(url, strain_name=None, strain_attrs=None):\n",
    "    successful = False\n",
    "    while not successful:\n",
    "        try:\n",
    "            response = requests.get(url)#, headers=headers, proxies=proxies)\n",
    "            if strain_name is None and strain_attrs is None:\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "            elif strain_attrs is None:\n",
    "                strainer = SoupStrainer(strain_name)\n",
    "                soup = BeautifulSoup(response.text, 'lxml', parse_only=strainer)\n",
    "            else:\n",
    "                strainer = SoupStrainer(strain_name, strain_attrs)\n",
    "                soup = BeautifulSoup(response.text, 'lxml', parse_only=strainer)\n",
    "            successful = True\n",
    "        except BaseException as e:\n",
    "            continue\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://araneae.nmbe.ch/'\n",
    "families_url = base_url + 'list/families'\n",
    "\n",
    "main_page = get_soup_simple(families_url, 'tbody')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "families = [x.text for x in main_page.tbody('b')]\n",
    "links_to_genera = [base_url+x['href'] for x in main_page.tbody('a') if x.text.lower() == 'genera']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "info = []\n",
    "\n",
    "for family, link_to_genera in tqdm(zip(families, links_to_genera), total=len(families)):\n",
    "    sleep(0.1)\n",
    "    f_data = get_soup_simple(link_to_genera, 'tbody')\n",
    "    genera = [x.text for x in f_data.tbody('b')]\n",
    "    links_to_species = [base_url+x['href'] for x in f_data.tbody('a') if x.text.lower() == 'species']\n",
    "    for genus, link_to_species in tqdm(zip(genera, links_to_species), total=len(genera), leave=False):\n",
    "        sleep(0.1)\n",
    "        g_data =  get_soup_simple(link_to_species, 'tbody')\n",
    "        all_species = [genus + ' ' + x.text for x in g_data.tbody('em')][1:]\n",
    "        species_links = [base_url+x['href'] for x in g_data('a') if x.text.lower() == 'data sheet']\n",
    "        for species, species_link in zip(all_species, species_links):\n",
    "            info.append([family, genus, species, species_link])\n",
    "info = np.array(info)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(info, columns=['family', 'genus', 'species', 'link'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.to_json('data/spiderbase_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/spiderbase_1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_images(img_content, species):\n",
    "    groups = {'habitus': [], 'epigyne': [], 'pedipalp': [], 'vulve': [], 'other': []}\n",
    "    for img in img_content:\n",
    "        name = img['name'].lower()\n",
    "        if 'epigyne' in name:\n",
    "            groups['epigyne'].append(img)\n",
    "        elif 'pedipalp' in name:\n",
    "            groups['pedipalp'].append(img)\n",
    "        elif 'vulve' in name:\n",
    "            groups['vulve'].append(img)\n",
    "        elif 'habitus' in name:\n",
    "            groups['habitus'].append(img)\n",
    "        else:\n",
    "            groups['other'].append(img)\n",
    "    return groups\n",
    "    \n",
    "def get_image_metainfo_quick(args):\n",
    "    url, species_name = args\n",
    "    img_content = []\n",
    "    sleep(0.1)\n",
    "    try:\n",
    "        img_data = get_soup_simple(url, 'div', {'class': 'thumbnail'})('div', 'thumbnail')\n",
    "        for dt in img_data:\n",
    "            sleep(0.1)\n",
    "            name = dt.p.text.lstrip()\n",
    "            credits = dt.a['title']\n",
    "            link = base_url+dt.img['src']\n",
    "            image = io.imread(link)\n",
    "            img_content.append({'name': name, 'image': image , 'credits': credits})\n",
    "\n",
    "        groups = group_images(img_content, species_name)\n",
    "        with open(f'data/image_data_1/{species_name.replace(\" \",\"-\").lower()}.pkl','wb') as out_file:\n",
    "            pickle.dump(groups, out_file)\n",
    "        return True, args, ''\n",
    "    except Exception as e:\n",
    "        return False, args, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b6987cfb834b219b9c7a89005b3ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5235.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arg_list= df[['link', 'species']].values.astype(str)\n",
    "\n",
    "results = []\n",
    "for arg in tqdm(arg_list):\n",
    "    results.append(get_image_metainfo_quick(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
